---
title: "from the <br/>Notebook<br/>to the<br/>Cluster"
author: "Erwin Lares"
affiliation: "Research Cyberinfrastructure at DoIT"
date: "`r format(Sys.time(), '%d %B %Y')`"
format: 
  html:
    toc: true
    toc-depth: 4
    code-fold: true
execute: 
  echo: false
  message: false
  warning: false
  
---

```{r}
#| label: setup


library(knitr)
library(tidyverse)
library(gt)
library(pins)
library(tictoc)
library(tidymodels)
library(janitor)
library(googlesheets4)
library(googledrive)
library(RSQLite)
library(quarto)
library(here)



```

![](images/splash_page.png)

## What this is about {#sec-page}

What happens when your data grows too large for your laptop? That was the question that gave rise to this project we call _"From the Notebook to the Cluster"_.

My goal, for the first half, is to create a walk-through that documents the steps I took to load the data, analyze it, and produce research artifacts. In the second half, I document the changes to the workflow so that the analysis can be shipped to an external cluster. 



Along the way, I will show the rationale behind my design choices. I'll also showcase common best practices as well as tools available to you which might make your work easier. 


# The notebook half  

## 1 - A workflow running on a laptop

This project describes the *Penguin Research Project* (PRP).  PRP's goal is to expand on the initial observations done by Dr. Gorman's in 2007. To that effect, I'll showcase an end-to-end workflow that results in a variety of research artifacts. 

@fig-roadmap outlines ...

```{mermaid}
%%| fig-width: 8
%%| label: fig-roadmap

flowchart LR
  A[Original<br/>Research] --> B(PRP)
  B --> C[running  <br/>on laptop  ]
  B --> D[running  <br/>on Workbench  ]
  B --> E[running on<br/>an external cluster  ]

```

The general idea behind PRP is to provide you with a templated workflow that is general enough so many of the individual steps required to complete the project can be adopted to researchers from multiple fields. 

The workflow outlined by PRP involves digesting and cleaning data, computing summarizing statistics,  creating visualizations, communicating results, fitting a classification model, and running predictions on new data.

There are compelling reasons to run one's analysis on a personal computer. They are accessible, portable and modern laptops are powerful devices capable of crunching lots of data.

However, as we will see, when the data start to increase in orders of magnitude or the computing needs of one's analyses start to require hours needed to run, it might be advisable to look for resources with larger compute capabilities. 

### Ingest the data

The first step is to get the data into whatever tool we intent to use for our analysis. For the first iteration of this tutorial, it is the R programming language. Future iterations of this document will include a Python version.

There are numerous ways in which data can be digested. This guide shows a few options. After each method of reading data is completed, the elapsed time is provided to give an idea how long it takes to complete the task.

#### Data stored as a local flat file

This involves storage that is physically connected to your machine. Typically we are talking about hard drives permanently connected to your device and removable storage such as external hard drives and USB-drives. 

```{r}
#| echo: true
#| message: false

tic(msg = "local csv")
data <- read_csv(file = here("data/original_palmerpenguins.csv"))
toc()
```

#### Data distributed as part of a package. 

Another way in which data is commonly distributed is as part of a package or library. Technically, it is still local: at one time, you have had to download it to your computer via `install.package()` for instance. 

Data distributed in this manner is easy to share with collaborators and it is very quick to access. Typically, we are talking about small datasets though.

```{r}
#|echo: true
#|message: false

tic(msg = "csv from a package")
data <- palmerpenguins::penguins
toc()
```

#### Data read from a database 

As data grows or multiples users require access to the same data, it is common to host the data in a database, rather than a set of flat files. This method involves a few extra steps, but the potential gains are enormous.

Things to keep an eye out for:  
- a database connection needs to be established.  
- each type of database required its own driver.  
- it is advisable to close the connection to the database after you are done. 


```{r}
#| echo: true

con <- dbConnect(SQLite(), here("data/palmerpenguins_db"))
tic(msg = "csv from a database")
data <- dbGetQuery(con, "select * from original_penguins")
toc()
dbDisconnect(con)
```

::: {.callout-tip collapse="true" title="Choosing the right `read*csv()`"}

When reading csv files, be mindful that size does matter. For smaller size files, go with `read.csv()`. Below around 1 MB `read.csv()` is actually faster than `read_csv()` 

:::



#### Data using a *Globus* endpoint

The data may exist on a different server physically located next to you or on the other side of the planet. We recommend  `Globus` to facilitate data sharing. `Globus` allows users to create _endpoints_ to that allows collaborators access to that data. 

In the example below, the data is access through an endpoint set up by PRP's scientist on a remote location. 

```{r}
#| echo: true
#| message: false 

tic(msg = "csv from a Globus endpoint")
data <- read_csv("https://g-394ce9.dtn.globus.wisc.edu/public_data/palmerpenguins.csv")
toc()

```

::: callout-important
Learn more about storage and sharing data with [ResearchDrive and Globus at UW-Madison](https://researchci.it.wisc.edu/).
:::

#### Data from an online source 

Here the online source is the flat file stored on a Google Drive folder made accessible through through a public link. Depending on the type of data you are working with, some options may or may not be available to you. 

```{r}
#| echo: true
#| message: false

tic(msg = "csv from an online source")
data <- read_csv("https://go.wisc.edu/9pv6q1")
toc()

```

#### Other posibilities not explored (yet)

-   manual entry (avoid this like the plague)   
-   instruments (using a service account)   
-   data repositories   
-   S3 buckets


## Analysis running on a laptop

@fig-mermaid-workflow details the individual steps of this workflow.

As mentioned earlier, these steps are common among most workflows so that regardless of your field, you would probably need to follow some of these steps in some shape or form.   

```{mermaid}
%%| fig-width: 8
%%| label: fig-mermaid-workflow

flowchart LR
  A(ingest<br/>data) --> B(clean & prep<br/>data)
  B --> C(summarize<br/>data)
  C --> D(visualize<br/>data)
  D --> E(communicate<br/>findings)
  E --> F(calculations)
  F --> G(model<br/>data)
  G --> H(predict)
  H --> I(save artifacts)
```

### Ingesting the data

From all the options listed in the previous section, we'll model grabbing the data from an online source, which is repeated below.


```{r}
#| echo: true

#here the online source is the flat file store on a Google Drive folder
#through a public link

data <- read_csv("https://go.wisc.edu/9pv6q1")

```

### Cleaning and Data prep 

Once the data is ingested, it often requires cleaning, reshaping, or creating new variables to complete a particular analysis. 

In this case, we would adhere to the [Style Guide from Advanced R](http://adv-r.had.co.nz/Style.html). We'll change variable names to *snake_case*, make them more descriptive and we'll get rid of some unnecessary variables. 

#### a - cleaning names

The code cleans the variable names manually by choosing what variables to keep with `select()`, then renaming them with `mutate()`, finally all missing values are dropped with `na_omit()` and only the new variables are kept. 

For an alternative way to accomplish this, consider using the `clean_names()` function from the `janitor` package which implements the _Style Guide_ mentioned above.  

```{r}
#| echo: true
data <- data |>
    select(Species, 
           Island,
           `Culmen Length (mm)`,
           `Culmen Depth (mm)`,
           `Flipper Length (mm)`,
           `Body Mass (g)`, 
           Sex,
           `Date Egg`) |>
    mutate(species = word(Species, 1), 
           island = Island,
           bill_length_mm = `Culmen Length (mm)`,
           bill_depth_mm = `Culmen Depth (mm)`,
           flipper_length_mm = `Flipper Length (mm)`,
           body_mass_g = `Body Mass (g)`, 
           sex = Sex,
           year = year(`Date Egg`)) |> 
    select(species, 
           island,
           bill_length_mm,
           bill_depth_mm,
           flipper_length_mm,
           body_mass_g, 
           sex,
           year) |> 
  na.omit()

```

#### b - creating new variables 

As part of an analysis, we may need to create new variables. In our case, we want to classify each specimen as tiny, small, medium, or large depending on their body mass. We will follow this criterion: if a penguin is below the 1st quartile, that penguin is _tiny_. Above the 1st quartile and at or below the 2nd quartile, _small_. Above the 2nd quartile and at or below the 3rd quartile, _medium_ Finally, any specimen above the 3th will be classified as _large_. 

- tiny: 0% < penguin <= 25%
- small: 25% < penguin <= 50%
- medium: 50% < penguin <= 75%
- large: 75% < penguin <= 100%

One possible way to accomplish this is detailed below.

```{r}
#| echo: true  
p_quartiles <- function(df) {
#p-quartiles() takes a df and bins the observations into the quartiles 
    df |> 
    select(body_mass_g) |> 
    as_vector() |> 
    quantile(probs = c(.25, .5, .75, 1),na.rm = TRUE)
}

quartile_data <- data |>
    split(data$species) |> 
#  group_split(species) |> #loses the grouping values 
    map(p_quartiles) |>
    bind_rows(.id = "id") |> 
    rename(species = id)



```
The resulting table, named `quartile data` contains the cutout values segregated by species. 

```{r}
#| echo: true
quartile_data |> 
    gt() |>
    tab_header("Cutout values for weight per species")
    
```

We will create a new variable `size` of type factor which contains the size label of each specimen.

```{r}
#| echo: true
data <- data |> 
    group_by(species) |> 
    mutate(size = case_when(
        body_mass_g <= quartile_data[[cur_group_id(),2]] ~ "tiny",
        body_mass_g <= quartile_data[[cur_group_id(),3]] ~ "small",
        body_mass_g <= quartile_data[[cur_group_id(),4]] ~ "medium",
        body_mass_g > quartile_data[[cur_group_id(),4]] ~ "large")) |>
    ungroup() |> 
    mutate(size = factor(size, levels = c("tiny", "small", "medium", "large")))
```

#### c - reshaping data 

Reshaping data is a very common step you may need complete in order to massage your data into a form that that is better suited for your analysis. The _shape_ of the data refers to whether the data is in a _long_ format or _wide_ format. Wide format is often used when analyzing data, performing calculations, or manual data entry. It’s intuitive for reading and interpreting data. 

By comparison, long format is used  when visualizing multiple variables in plots using statistical software. Long format is essential for creating specific types of visualizations. 

You can read more about this topic [here](https://towardsdatascience.com/long-and-wide-formats-in-data-explained-e48d7c9a06cb).


### Communicating findings

An analysis can result in a number of _research artifacts_. Common among those are tables, visualization, and models. These artifacts can be subsequently shared in a variety of formats such as printed documents, online documents, dashboards, and APIs just to name a few.


#### Summarizing findings via tables 


Tables are a concise way to  systematically organize large amounts of data, condensing complex details into a more manageable form that is easy to access. 

There is a variety of choices to create great looking tables in R. One can go around in a manual manner using `markdown` syntax or the visual editor if working with RStudio. I prefer to approach this task in a programmatic manner. For this document, I will use the `gt` package. 

The goal for this table is to display the number and percentages of small, medium, and large penguins per species. 

```{r penguins-per-size-and-species}
#| echo: true

data |> 
    select(species, size) |>
    group_by(species) |> 
    reframe(count = fct_count(size |> as_vector())) |> 
    unnest(cols = c(count)) |> 
    rename(size = f,
           count = n) |> 
    group_by(species) |> 
    mutate(n = sum(count),
           percentage = round(count/n*100, 2)) |> 
    gt() |>
    tab_header("Sizes per species")

```

#### Visualize the data

>> A picture is worth a thousand words

@fig-bloxplots shows the weight distribution of all three species of Penguins, discriminated by location on which each specimen was captured. Gentoo penguins are clearly the largest species of penguins in the study, whereas Adelie penguins are the smallest of the three species. Adelie penguins, unlike Gentoos and Chinstraps, are found in all three islands surveyed. By comparison, Gentoos are only found in the Biscoe Island, and Chinstraps are only found in the Dream island. 


```{r}
#| echo: true
#| label: fig-boxplots

data |> 
  ggplot(aes(x = species, y = body_mass_g, color = species)) +
  geom_jitter(alpha = 0.25) +
  geom_boxplot(alpha = 0.25) +
  theme_minimal() + 
  facet_grid( ~ island) +
  xlab("body mass in g")

```

#### Reporting and communicating findings 

It is often the case that after a research cycle has been completed, a comprehensive report needs to be created to document the methods, analyses and findings. Sometimes research workflows may require to produce reports periodically, i.e. the data changes, focused reports for specific audiences, changes in methods, etc. 

A solution that allows for prose and code on the same document is an excellent option for creating documents that build their content on components that are subject to change. This very document you are reading is an example of this approach. 

As a prototype, let's consider creating a document that reports the some of the content outlined earlier for a specific type of penguin. The document includes a brief paragraph, a table, and a visualization. The document also includes a parameter that specifies the type of penguin we want to create a report for. 

A Quarto document, like the one you're are reading right now combines prose and code. The prose is a flavor of `markdown`. Quarto can work with a variety of languages such as `R`, `Python`, and `ObservableJS` among others. 

The first required part of Quarto document is its `yaml` header which starts and ends with three dashes like so`---`.  The `yaml` header contains metadata needed to run the document. Among the data, we will include a `params` argument that is defaulted to "Adelie" but it can be changed at run time. 

The file template is housed under the `reports` folder and it is named `penguin-summary.qmd`. Its `yaml` header looks like this

```{}
---
title: "Penguin Summary"
format: html
params:
  species: "Adelie"
---

```

The rest of the document is a combination of prose and code need to convey the report's content. The code below uses the `quarto_render()` function. It takes as arguments the location of the templated file, the desired output format, and a list containing the species parameter. Quarto will perform some calculations that include the number of specimens, the maximum, minimum, and mean weight of the sample discriminated by the species parameter and knit it together with the text to produce a finalized report.

The code below can be adjusted at runtime to produce a report _different_ from the default value. This can be very useful to create reports targeted to a specific stakeholder audience. For instance, the code changed from the default value "Adelie" to "Chinstrap". 

```{r render-parametrized-report}
#| eval: false


quarto_render( here("reports/penguin-summary.qmd"), 
               output_format = "pdf",
               execute_params = list(species = "Chinstrap"))

#check this post 
#https://stackoverflow.com/questions/73571919/how-to-pass-logical-parameters-with-the-quarto-r-package-to-the-knitr-chunk-opti



```

### Performing calculations

Oftentimes part of an analyses workflow involves repeating a calculations across subsets of the data. For this use case, we are interested in the measures of tendency for each of the species of penguins in the dataset.

- mbl: mean bill length in mm  
- sdbl: standard deviation of bill length  
- mbd: mean bill depth in mm  
- sdbd: standard deviation of bill depth  
- mfl: mean flipper length in mm  
- sdfl: standard deviation of flipper length  
- mbm: mean body mass in g  
- sdbm: standard deviation of body mass  



Below I show different approaches to this problem. From worst to --- arguably --- best.

#### Method 1: explicit calculations

Explicit calculations entails splitting the data into the relevant subsets - here each species - and carry out the required calculations for each subgroup. 



```{r}
#| echo: true
#| label: manual-ling 


# mbl: mean bill length in mm 
# sdbl: standard deviation of bill length
# mbd: mean bill depth in mm
# sdbd: standard deviation of bill depth
# mfl: mean flipper length in mm
# sdfl: standard deviation of flipper length
# mbm: mean body mass in g
# sdbm: standard deviation of body mass

tic.clear()
tic("Manual calculations") 

data |> 
  filter(species == "Adelie") |> 
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

data |> 
  filter(species == "Gentoo") |> 
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

data |> 
  filter(species == "Chinstrap") |> 
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

manual_time <- toc()


```

::: {.callout-important collapse="true" title="The DRY Principle"}

> "Don't repeat yourself" (DRY) is a principle of software development aimed at reducing repetition of software patterns, replacing it with abstractions or using data normalization to avoid redundancy.

<!-- >The DRY principle is stated as "Every piece of knowledge must have a single, unambiguous, authoritative representation within a system". The principle has been formulated by Andy Hunt and Dave Thomas in their book The Pragmatic Programmer. They apply it quite broadly to include database schemas, test plans, the build system, even documentation. When the DRY principle is applied successfully, a modification of any single element of a system does not require a change in other logically unrelated elements. Additionally, elements that are logically related all change predictably and uniformly, and are thus kept in sync. Besides using methods and subroutines in their code, Thomas and Hunt rely on code generators, automatic build systems, and scripting languages to observe the DRY principle across layers. -->

> Definition taken from Wikipedia on 2023/04/19

:::

#### Applying the DRY principle

Carrying out calculations manually can be costly in time; even with cut and paste! In addition, manually copying code is prone to errors and typos and is hard to maintain. 

On the other hand, parsimonious code is very explicit in what it does.


#### Method 2: for-loops

If you look closely at the code showed before, you'll see that a good part of the code in all the calculations is shared. A better choice is to use a for-loop ranging over the penguin species in the dataset.


```{r}
#| echo: true
#| label: looping-ly
#| include: true
#| code-line-numbers: "4, 7-15"

tic.clear()
tic("For-loop calculations") 

for (penguin in c("Adelie", "Gentoo", "Chinstrap")) {
  data |> 
  filter(species == penguin) |> 
  summarize(species = unique(species),
            mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            ) |> 
    print()

}
  
forloop_time <- toc()


```


#### Method 3: using `dplyr` syntax to manage grouping and aggregation

Another option is to take advantage of `tidyverse's dplyr` syntax to deal with subsetting and aggregation. One of the advantages of `dplyr` syntax is how human-readble it is. 




```{r}
#| echo: true
#| label: dplying


tic.clear()
tic("Dplyr calculations")

data |> 
  group_by(species) |> 
  summarize(mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            )

dplyr_time <- toc()



```

#### Method 4: abstracting with the purrr package

We can take a DRY-er approach by encapsulating the calculations using a function and applying said function to each of the relevant subset. The good news is that the code that makes up the function is practically written already. The two necessary steps to complete this workflow are making the function explicit and deciding what arguments the function needs to take. 


```{r}
#| echo: true

#encapsulating the code in a function
#the function takes the subset of the data as an dataframe argument 

summarize_penguin_population <- function(df) {

  df |>
    summarize(
      species = unique(species),
      mbl = mean(df$bill_length_mm),
      sdbl = sd(df$bill_length_mm),
      mbd = mean(df$bill_depth_mm),
      sdbd = sd(df$bill_depth_mm),
      mfl = mean(df$flipper_length_mm),
      sdfl = sd(df$flipper_length_mm),
      mbm = mean(df$body_mass_g),
      sdbm = sd(df$body_mass_g))

}


```


::: callout-important
-   There is an additional step involved: turning the groups into `list` objects using the `group_split()` function.
-   The function `map()` is the magic sauce, it requires as arguments the individual lists and a function to be applied to each element of the list.
:::



```{r}
#| echo: true
#| label: purrr-ing


tic.clear()
tic("Map calculations") 

#this was my initial thought, 
data |>
  group_by(species) |>
  group_split() |>
  map(summarize_penguin_population) |> 
  bind_rows()

# data |>
#     split(data$species) |> 
#     map(summarize_penguin_population) |> 
#     bind_rows()

# an even shorter code relying on the dataframe specific map_df() functon 
# data |>
#     group_split(data$species) |>
#     map_df(summarize_penguin_population)


map_time <- toc()

```

#### Comparing the four methods

Let's compare how long each method took to perform the calculations.


```{r}
#| include: true
#| label: calculations-by-method

#the factor function called and the explicit level assignment are needed to 
#prevent ggplot from reordering the values in alphabetic order

times_by_method <- tibble(method = factor(c("manual_ly",
                                      "forloop_ly",
                                      "group_by_ly",
                                      "map_ly"),
                                    levels = c("manual_ly",
                                      "forloop_ly",
                                      "group_by_ly",
                                      "map_ly")), 
                           duration = c(manual_time[[2]] - manual_time[[1]],
                                        forloop_time[[2]] - forloop_time[[1]],
                                        dplyr_time[[2]] - dplyr_time[[1]],
                                        map_time[[2]] - map_time[[1]] )) 
times_by_method |> 
  gt() |> 
  tab_header(
    title = "Time for calculations by method")

times_by_method |> 
  ggplot(aes(x = method, y = duration, fill = method))+
  geom_col() +
  ylab("duration in seconds")

```


There is a tension between clarity and performance. One can be as parsimonious as possible, explicitly writing down every calculation we want carried out, but that comes with loss of performance and a propensity to make mistakes. The `dplyr` and `map` options require a bit of abstracting away from the individual steps. They also required additional steps in order to be implemented, such as creating the function to be mapped and splitting the data into the appropriate subsets. 


#### What's the better approach? 

We have explored iterating using for-loops, dplyr syntax, and purrr's map functions. Each approach has advantages and disadvantages. For instance, I prefer the purrr and dplyr's approaches because of its readability. Ultimately, the size of your data, the requirements of your calculations, or the prospects of maintaining the code might tip you in one direction over others.

Finally, the four approaches above are not an exhaustive list of your choices. For dplyr-like approaches that are more performant check the `datatable` package. For purrr-like approaches, you can also explore the `apply` family of function from base R.


### Creating a predictive model

The original Palmer penguins were distributed among three islands. For our simulation, we are going to explore the scenario were *Climate Change* has forced these three species of penguins to move to _Janus_ island, a location further south that still maintain the temperature the penguins are accustomed to. 


Penguins researchers are interested in classifying the wave of penguins migrating to Janus island. The classification model is based on the original data set.



```{r}
#| echo: true
#| include: true



filtered_penguins <- read_csv(here("data/original_palmerpenguins.csv")) |>
  drop_na(ends_with("mm")) |> 
  select(species, ends_with("mm"), body_mass_g) |> 
  mutate(species = as_factor(species))

tic.clear()
tic("Fitting model")

model <- rand_forest() |> 
  set_engine("ranger") |> 
  set_mode("classification") |> 
  fit(species ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g, data = filtered_penguins)

fit_time <- toc()

```



::: callout-note
With this fitted model in place, we can feed new penguins measurements to it and ask the model what species of penguins each new specimen is. 
:::



### Utilizing the model to predict new specimens

Our PRP collaborators are busy collecting and measuring penguins on Janus island. Those measurements are made available through a `Globus` endpoint

Using the model created earlier to predict what species of a new penguin specimen is is easy. The code below reads the new data from Janus island
```{r}
# sim_data <- googlesheets4::read_sheet("https://docs.google.com/spreadsheets/d/1HEQa23hR202egPcnyAtQHGyPRw1WWv61H6aHnAhjcrc/edit#gid=519943868")

janus_penguins <- read_csv("https://g-394ce9.dtn.globus.wisc.edu/public_data/1M_penguins.csv")


tic()

predict(model,
        new_data = slice_sample(.data = head(janus_penguins),
                                n = 10,
                                replace = FALSE),
        type = "prob")  #type = "class" returns a the type of penguin rather than the prob.,

toc()
```


```{r}
sim_data_10 <- slice_sample(janus_penguins, n = 10)

tic("10 observations")
sim_data_10 |>
bind_cols(predict(model, new_data = sim_data_10)) |>
  count(species, .pred_class) |> 
  mutate(sample_size = nrow(sim_data_10))
obs_10 <- toc()
```


### What happens when your data grows?


Not sure what the point of this plot is ...   s
```{r}
  ggplot() + 
  geom_density(mapping = aes(x = flipper_length_mm, color = species), 
               data = janus_penguins) +
    geom_density(mapping = aes(x = flipper_length_mm, color = species), 
               data = data)
```

---
:::callout-tip
### Benchmarking summary calculations

The next number of calculations are repeats of what we have done previously varying the number of observations. I will benchmark 10, 100, 1000, 10k, 50k, and 100k observation. The expectation is that as the number of observations increase, the time required to perform said calculations will increase. Eventually, if you are in a field that generates large amounts of data, running your analyses in your laptop becomes prohibitively long.

:::


```{r}
#| label: slicing 

sim_data_10 <- slice_sample(.data = janus_penguins, n = 10, replace = TRUE)
sim_data_100 <- slice_sample(.data = janus_penguins, n = 100, replace = TRUE)
sim_data_1k <- slice_sample(.data = janus_penguins, n = 1000, replace = TRUE)
sim_data_10k <- slice_sample(.data = janus_penguins, n = 10000, replace = TRUE)
sim_data_50k <- slice_sample(.data = janus_penguins, n = 50000, replace = TRUE)
sim_data_100k <- slice_sample(.data = janus_penguins, n = 100000, replace = TRUE)
sim_data_500k <- slice_sample(.data = janus_penguins, n = 500000, replace = TRUE)
sim_data_1M <-  slice_sample(.data = janus_penguins, n = 1000000, replace = TRUE)
```

## Using manual/explicit calculations

```{r}
#| echo: true
#| label: manualling-2 

#####################################
# 100 points of data


tic.clear()
tic("Manual calculations 100") 

sim_data_100 |> 
  filter(species == "Adelie") |> 
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

sim_data_100 |> 
  filter(species == "Gentoo") |> 
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

sim_data_100 |> 
  filter(species == "Chinstrap") |> 
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

manual_time_100 <- toc()

#####################################
# 1000 points of data

tic.clear()
tic("Manual calculations 1000") 

sim_data_1k |> 
  filter(species == "Adelie") |> 
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

sim_data_1k |> 
  filter(species == "Gentoo") |> 
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

sim_data_1k |> 
  filter(species == "Chinstrap") |> 
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

manual_time_1k <- toc()

#####################################
# 10000 points of data

tic.clear()
tic("Manual calculations 10000") 

sim_data_10k |> 
  filter(species == "Adelie") |> 
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

sim_data_10k |> 
  filter(species == "Gentoo") |> 
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

sim_data_10k |> 
  filter(species == "Chinstrap") |> 
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

manual_time_10k <- toc()

#####################################
# 50000 points of data

tic.clear()
tic("Manual calculations 50k") 

sim_data_50k |> 
  filter(species == "Adelie") |> 
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

sim_data_50k |> 
  filter(species == "Gentoo") |> 
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

sim_data_50k |> 
  filter(species == "Chinstrap") |> 
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

manual_time_50k <- toc()

#####################################
# 100,000 points of data
# it took 3667.102 sec to simulate this on my laptop
# cci-workbench took 3693.41 sec elapsed. ACTUALLY LONGER 


tic.clear()
tic("Manual calculations 100k")
sim_data_100k |>
  filter(species == "Adelie") |>
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

sim_data_100k |>
  filter(species == "Gentoo") |>
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

sim_data_100k |>
  filter(species == "Chinstrap") |>
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

manual_time_100k <- toc()

#####################################
# 500,000 points of data
# it took 3667.102 sec to simulate this on my laptop
# cci-workbench took 3693.41 sec elapsed. ACTUALLY LONGER 


tic.clear()
tic("Manual calculations 500k")
sim_data_500k |>
  filter(species == "Adelie") |>
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

sim_data_500k |>
  filter(species == "Gentoo") |>
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

sim_data_500k |>
  filter(species == "Chinstrap") |>
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

manual_time_500k <- toc()

#####################################
# 1,00,000 points of data
# it took 3667.102 sec to simulate this on my laptop
# cci-workbench took 3693.41 sec elapsed. ACTUALLY LONGER 


tic.clear()
tic("Manual calculations 1M")
sim_data_1M |>
  filter(species == "Adelie") |>
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

sim_data_1M |>
  filter(species == "Gentoo") |>
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

sim_data_1M |>
  filter(species == "Chinstrap") |>
  summarize(
    species = unique(species),
    mbl = mean(bill_length_mm),
    sdbl = sd(bill_length_mm),
    mbd = mean(bill_depth_mm),
    sdbd = sd(bill_depth_mm),
    mfl = mean(flipper_length_mm),
    sdfl = sd(flipper_length_mm),
    mbm = mean(body_mass_g),
    sdbm = sd(body_mass_g)
    )

manual_time_1M <- toc()


```

```{r}
#| include: true
#| label: calculations-times-manual-2

calculations_times_manual <- tibble(method = factor(c("100 data points",
                  "1k data points",
                  "10k data points",
                  "50k data points",
                  "100k data points",
                  "500k data points",
                  "1M data points"),
                  levels = c("100 data points",
                  "1k data points",
                  "10k data points",
                  "50k data points",
                  "100k data points",
                  "500k data points",
                  "1M data points")), 
       duration = c(manual_time_100[[2]] - manual_time_100[[1]],
                    manual_time_1k[[2]] - manual_time_1k[[1]], 
                    manual_time_10k[[2]] - manual_time_10k[[1]],
                    manual_time_50k[[2]] - manual_time_50k[[1]],
                    manual_time_100k[[2]] - manual_time_100k[[1]],
                    manual_time_500k[[2]] - manual_time_500k[[1]],
                    manual_time_1M[[2]] - manual_time_1M[[1]]))

calculations_times_manual |> 
  gt() |> 
  tab_header(
    title = "Calculation time as data increases",
    subtitle = "Calculation method: manual")

calculations_times_manual  |> 
  ggplot(aes(x = method, y = duration, fill = method))+
  geom_col() +
  ylab("duration in seconds") +
  coord_flip() + 
  scale_x_discrete(limits = rev(levels(calculations_times_manual$method)))

```

## Using for-loops

```{r}
#| echo: true
#| label: looping-2
#| include: true



######################################
# 10 data points

tic.clear()
tic("For-loop-10") 

for (penguin in c("Adelie", "Gentoo", "Chinstrap")) {
  sim_data_10 |> 
  filter(species == penguin) |> 
  summarize(species = unique(species),
            mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            ) |> 
    print()

}
  
forloop_time_10 <- toc()

######################################
# 100 data points

tic.clear()
tic("For-loop-100") 

for (penguin in c("Adelie", "Gentoo", "Chinstrap")) {
  sim_data_100 |> 
  filter(species == penguin) |> 
  summarize(species = unique(species),
            mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            ) |> 
    print()

}
  
forloop_time_100 <- toc()

######################################
# 1000 data points

tic.clear()
tic("For-loop-1k") 

for (penguin in c("Adelie", "Gentoo", "Chinstrap")) {
  sim_data_1k |> 
  filter(species == penguin) |> 
  summarize(species = unique(species),
            mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            ) |> 
    print()

}
  
forloop_time_1k <- toc()

######################################
# 10000 data points

tic.clear()
tic("For-loop-10k") 

for (penguin in c("Adelie", "Gentoo", "Chinstrap")) {
  sim_data_10k |> 
  filter(species == penguin) |> 
  summarize(species = unique(species),
            mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            ) |> 
    print()

}
  
forloop_time_10k <- toc()

######################################
# 50000 data points

tic.clear()
tic("For-loop-50k") 

for (penguin in c("Adelie", "Gentoo", "Chinstrap")) {
  sim_data_50k |> 
  filter(species == penguin) |> 
  summarize(species = unique(species),
            mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            ) |> 
    print()

}
  
forloop_time_50k <- toc()

######################################
# 100000 data points

tic.clear()
tic("For-loop-100k") 

for (penguin in c("Adelie", "Gentoo", "Chinstrap")) {
  sim_data_100k |> 
  filter(species == penguin) |> 
  summarize(species = unique(species),
            mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            ) |> 
    print()

}
  
forloop_time_100k <- toc()

######################################
# 500000 data points

tic.clear()
tic("For-loop-500k") 

for (penguin in c("Adelie", "Gentoo", "Chinstrap")) {
  sim_data_500k |> 
  filter(species == penguin) |> 
  summarize(species = unique(species),
            mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            ) |> 
    print()

}
  
forloop_time_500k <- toc()

######################################
# 1000000 data points

tic.clear()
tic("For-loop-1M") 

for (penguin in c("Adelie", "Gentoo", "Chinstrap")) {
  sim_data_1M |> 
  filter(species == penguin) |> 
  summarize(species = unique(species),
            mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            ) |> 
    print()

}
  
forloop_time_1M <- toc()

```

## Calculations - for-loop method 
```{r}
#| include: true
#| label: calculations-times-for-loop-2



calculations_times_forloop <- tibble(method = factor(c("100 data points",
                  "1k data points",
                  "10k data points",
                  "50k data points",
                  "100k data points",
                  "500k data points",
                  "1M data points"),
                  levels = c("100 data points",
                  "1k data points",
                  "10k data points",
                  "50k data points",
                  "100k data points",
                  "500k data points",
                  "1M data points")), 
       duration = c(forloop_time_100[[2]] - forloop_time_100[[1]],
                    forloop_time_1k[[2]] - forloop_time_1k[[1]], 
                    forloop_time_10k[[2]] - forloop_time_10k[[1]],
                    forloop_time_50k[[2]] - forloop_time_50k[[1]],
                    forloop_time_100k[[2]] - forloop_time_100k[[1]],
                    forloop_time_500k[[2]] - forloop_time_500k[[1]],
                    forloop_time_1M[[2]] - forloop_time_1M[[1]]))

calculations_times_forloop |> 
  gt() |> 
  tab_header(
    title = "Calculation time as data increases",
    subtitle = "Calculation method: for-loop")

calculations_times_forloop  |> 
  ggplot(aes(x = method, y = duration, fill = method))+
  geom_col() +
  ylab("duration in seconds") +
  coord_flip() + 
  scale_x_discrete(limits = rev(levels(calculations_times_forloop$method)))

```

## Calculations using dplyr syntax

```{r}
#| echo: true
#| label: dplying-2



########################################
# 10 points of data

tic.clear()
tic("Dplyr calculations 10 points of data")

sim_data_10 |> 
  group_by(species) |> 
  summarize(mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            )

dplyr_time_10 <- toc()

########################################
# 100 points of data

tic.clear()
tic("Dplyr calculations 10 points of data")

sim_data_100 |> 
  group_by(species) |> 
  summarize(mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            )

dplyr_time_100 <- toc()

########################################
# 1000 points of data

tic.clear()
tic("Dplyr calculations 1000 points of data")

sim_data_1k |> 
  group_by(species) |> 
  summarize(mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            )

dplyr_time_1k <- toc()

########################################
# 10000 points of data

tic.clear()
tic("Dplyr calculations 10000 points of data")

sim_data_10k |>  
  group_by(species) |> 
  summarize(mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            )

dplyr_time_10k <- toc()

########################################
# 50000 points of data

tic.clear()
tic("Dplyr calculations 50000 points of data")

sim_data_50k |> 
  group_by(species) |> 
  summarize(mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            )

dplyr_time_50k <- toc()


########################################
# 100000 points of data

tic.clear()
tic("Dplyr calculations 100000 points of data")

sim_data_100k |>
  group_by(species) |> 
  summarize(mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            )

dplyr_time_100k <- toc()

########################################
# 500000 points of data

tic.clear()
tic("Dplyr calculations 500000 points of data")

sim_data_500k |>
  group_by(species) |> 
  summarize(mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            )

dplyr_time_500k <- toc()


########################################
# 100000 points of data

tic.clear()
tic("Dplyr calculations 1M points of data")

sim_data_1M |>
  group_by(species) |> 
  summarize(mbl = mean(bill_length_mm),
            sdbl = sd(bill_length_mm),
            mbd = mean(bill_depth_mm),
            sdbd = sd(bill_depth_mm),
            mfl = mean(flipper_length_mm),
            sdfl = sd(flipper_length_mm),
            mbm = mean(body_mass_g),
            sdbm = sd(body_mass_g)
            )

dplyr_time_1M <- toc()

```


Results of dplyr calculations 

## Calculations - dplyr method 
```{r}
#| include: true
#| label: calculations-times-for-dplyr-2

tibble(duration = c(dplyr_time_100,
                    dplyr_time_1k, 
                    dplyr_time_10k,
                    dplyr_time_50k,
                    dplyr_time_100k,
                    dplyr_time_500k,
                    dplyr_time_1M)) |> 
  gt() |> 
  tab_header(
    title = "Calculation time as data increases",
    subtitle = "Calculation method: for-loop")

calculations_times_dplyr <- tibble(method = factor(c("100 data points",
                  "1k data points",
                  "10k data points",
                  "50k data points",
                  "100k data points",
                  "500k data points",
                  "1M data points"),
                  levels = c("100 data points",
                  "1k data points",
                  "10k data points",
                  "50k data points",
                  "100k data points",
                  "500k data points",
                  "1M data points")), 
       duration = c(dplyr_time_100[[2]] - dplyr_time_100[[1]],
                    dplyr_time_1k[[2]] - dplyr_time_1k[[1]], 
                    dplyr_time_10k[[2]] - dplyr_time_10k[[1]],
                    dplyr_time_50k[[2]] - dplyr_time_50k[[1]],
                    dplyr_time_100k[[2]] - dplyr_time_100k[[1]],
                    dplyr_time_500k[[2]] - dplyr_time_500k[[1]],
                    dplyr_time_1M[[2]] - dplyr_time_1M[[1]]))

calculations_times_dplyr |> 
  gt() |> 
  tab_header(
    title = "Calculation time as data increases",
    subtitle = "Calculation method: dplyr")

calculations_times_dplyr  |> 
  ggplot(aes(x = method, y = duration, fill = method))+
  geom_col() +
  ylab("duration in seconds") +
  coord_flip() + 
  scale_x_discrete(limits = rev(levels(calculations_times_forloop$method)))

```



## Calculations with the map() function
```{r}
#| label: map-calculations 

############################
# 10 points of data 
tic.clear()
tic("Map calculations 10 points of data ") 

sim_data_10 |> 
  group_by(species) |>
  group_split() |>
  map(summarize_penguin_population) |> 
  list_rbind() 

map_time_10 <- toc()

############################
# 100 points of data 
tic.clear()
tic("Map calculations 100 points of data ") 

sim_data_100 |> 
  group_by(species) |>
  group_split() |>
  map(summarize_penguin_population) |> 
  list_rbind()

map_time_100 <- toc()

############################
# 1000 points of data 
tic.clear()
tic("map() calculations 1000 points of data ") 

sim_data_1k |> 
  group_by(species) |>
  group_split() |>
  map(summarize_penguin_population) |> 
  list_rbind()

map_time_1k <- toc()

############################
# 10000 points of data 
tic.clear()
tic("map() calculations 10000 points of data ") 

sim_data_10k |> 
  group_by(species) |>
  group_split() |>
  map(summarize_penguin_population) |> 
  list_rbind()

map_time_10k <- toc()

############################
# 50000 points of data 
tic.clear()
tic("map() calculations 50000 points of data ") 

sim_data_50k |> 
  group_by(species) |>
  group_split() |>
  map(summarize_penguin_population) |> 
  list_rbind()

map_time_50k <- toc()

############################
# 100000 points of data 
tic.clear()
tic("map() calculations 100000 points of data ") 

sim_data_100k |> 
  group_by(species) |>
  group_split() |>
  map(summarize_penguin_population) |> 
  list_rbind()

map_time_100k <- toc()

############################
# 100000 points of data 
tic.clear()
tic("map() calculations 100000 points of data ") 

sim_data_100k |> 
  group_by(species) |>
  group_split() |>
  map(summarize_penguin_population) |> 
  list_rbind()

map_time_100k <- toc()

############################
# 500000 points of data 
tic.clear()
tic("map() calculations 500000 points of data ") 

sim_data_500k |> 
  group_by(species) |>
  group_split() |>
  map(summarize_penguin_population) |> 
  list_rbind()

map_time_500k <- toc()

############################
# 1000000 points of data 
tic.clear()
tic("map() calculations 1000000 points of data ") 

sim_data_1M |> 
  group_by(species) |>
  group_split() |>
  map(summarize_penguin_population) |> 
  list_rbind()

map_time_1M <- toc()

```

## Calculations - map method 
```{r}
#| include: true
#| label: calculations-times-for-map-2



calculations_times_map <- tibble(method = factor(c("100 data points",
                  "1k data points",
                  "10k data points",
                  "50k data points",
                  "100k data points",
                  "500k data points",
                  "1M data points"),
                  levels = c("100 data points",
                  "1k data points",
                  "10k data points",
                  "50k data points",
                  "100k data points",
                  "500k data points",
                  "1M data points")), 
       duration = c(map_time_100[[2]] - map_time_100[[1]],
                    map_time_1k[[2]] - map_time_1k[[1]], 
                    map_time_10k[[2]] - map_time_10k[[1]],
                    map_time_50k[[2]] - map_time_50k[[1]],
                    map_time_100k[[2]] - map_time_100k[[1]],
                    map_time_500k[[2]] - map_time_500k[[1]],
                    map_time_1M[[2]] - map_time_1M[[1]]))

calculations_times_map |> 
  gt() |> 
  tab_header(
    title = "Calculation time as data increases",
    subtitle = "Calculation method: map")

calculations_times_map  |> 
  ggplot(aes(x = method, y = duration, fill = method))+
  geom_col() +
  ylab("duration in seconds") +
  coord_flip() + 
  scale_x_discrete(limits = rev(levels(calculations_times_map$method)))

```


## Model performance
```{r}

# sim_data_100 <- slice_sample(.data = janus_penguins, n = 100, replace = TRUE)
# sim_data_1000 <- slice_sample(.data = janus_penguins, n = 1000, replace = TRUE)
# sim_data_10000 <- slice_sample(.data = janus_penguins, n = 10000, replace = TRUE)
# sim_data_50000 <- slice_sample(.data = janus_penguins, n = 50000, replace = TRUE)
tic.clear()
tic("100 observations")
model_perf_100 <- sim_data_100 |>
bind_cols(predict(model, new_data = sim_data_100)) |> 
  count(species, .pred_class) |> 
  mutate(sample_size = nrow(sim_data_100),
         pred_species = .pred_class) |> 
  select(-.pred_class)
obs_100 <- toc()

tic.clear()
tic("1000 observations")
model_perf_1k <- sim_data_1k |>
bind_cols(predict(model, new_data = sim_data_1k)) |> 
  count(species, .pred_class) |> 
  mutate(sample_size = nrow(sim_data_1k),
         pred_species = .pred_class) |> 
  select(-.pred_class)
obs_1k <- toc()

tic.clear()
tic("10k observations")
model_perf_10k <- sim_data_10k |>
bind_cols(predict(model, new_data = sim_data_10k)) |> 
  count(species, .pred_class) |> 
  mutate(sample_size = nrow(sim_data_10k),
         pred_species = .pred_class) |> 
  select(-.pred_class)
obs_10k <- toc()

tic.clear()
tic("50k observations")
model_perf_50k <- sim_data_50k |>
bind_cols(predict(model, new_data = sim_data_50k)) |> 
  count(species, .pred_class) |> 
  mutate(sample_size = nrow(sim_data_50k),
         pred_species = .pred_class) |> 
  select(-.pred_class)
obs_50k <- toc()


tic.clear()
tic("100k observations")
model_perf_100k <- sim_data_100k |>
bind_cols(predict(model, new_data = sim_data_100k)) |> 
  count(species, .pred_class) |> 
  mutate(sample_size = nrow(sim_data_100k),
         pred_species = .pred_class) |> 
  select(-.pred_class)
obs_100k <- toc()

tic.clear()
tic("500k observations")
model_perf_500k <- sim_data_500k |>
bind_cols(predict(model, new_data = sim_data_500k)) |> 
  count(species, .pred_class) |> 
  mutate(sample_size = nrow(sim_data_500k),
         pred_species = .pred_class) |> 
  select(-.pred_class)
obs_500k <- toc()

tic.clear()
tic("1M observations")
model_perf_1M <- sim_data_1M |>
bind_cols(predict(model, new_data = sim_data_1M)) |> 
  count(species, .pred_class) |> 
  mutate(sample_size = nrow(sim_data_1M),
         pred_species = .pred_class) |> 
  select(-.pred_class)
obs_1M <- toc()


```

```{r}
#| include: true
#| label: calculations-model-performance



model_perf  <- tibble(method = factor(c("100 data points",
                  "1k data points",
                  "10k data points",
                  "50k data points",
                  "100k data points",
                  "500k data points",
                  "1M data points"),
                  levels = c("100 data points",
                  "1k data points",
                  "10k data points",
                  "50k data points",
                  "100k data points",
                  "500k data points",
                  "1M data points")), 
       miss_identifications = c(model_perf_100 |> filter(species != pred_species) |> pluck(2) |> sum(),
                                model_perf_1k |> filter(species != pred_species) |> pluck(2) |> sum(),
                                model_perf_10k |> filter(species != pred_species) |> pluck(2) |> sum(),
                                model_perf_50k |> filter(species != pred_species) |> pluck(2) |> sum(),
                                model_perf_100k |> filter(species != pred_species) |> pluck(2) |> sum(),
                                model_perf_500k |> filter(species != pred_species) |> pluck(2) |> sum(),
                                model_perf_1M |> filter(species != pred_species) |> pluck(2) |> sum()
                                ),
       perc_miss_id = c(model_perf_100 |> filter(species != pred_species) |> pluck(2) |> sum()/100*100,
                        model_perf_1k |> filter(species != pred_species) |> pluck(2) |> sum()/1000*100,
                        model_perf_10k |> filter(species != pred_species) |> pluck(2) |> sum()/10000*100,
                        model_perf_50k |> filter(species != pred_species) |> pluck(2) |> sum()/50000*100,
                        model_perf_100k |> filter(species != pred_species) |> pluck(2) |> sum()/100000*100,
                        model_perf_500k |> filter(species != pred_species) |> pluck(2) |> sum()/500000*100,
                        model_perf_1M |> filter(species != pred_species) |> pluck(2) |> sum()/1000000*100
                        ))

model_perf |> 
  mutate() |> 
  gt() |> 
  tab_header(
    title = "Model Performance")

calculations_times_map  |> 
  ggplot(aes(x = method, y = duration, fill = method))+
  geom_col() +
  ylab("duration in seconds") +
  coord_flip() + 
  scale_x_discrete(limits = rev(levels(calculations_times_map$method)))

```

The model does not degrade as the number of observations increases. It roughly misidentifies 5% of specimens.

# 2 - Running on Workbench

## What is _Workbench_?